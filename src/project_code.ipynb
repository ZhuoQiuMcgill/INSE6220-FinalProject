{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup: imports and paths\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Resolve raw and clean paths robustly relative to working directory\n",
    "RAW_CANDIDATES = [\n",
    "    os.path.join('data', 'koi.csv'),\n",
    "    os.path.join('..', 'data', 'koi.csv'),\n",
    "]\n",
    "RAW_PATH = next((p for p in RAW_CANDIDATES if os.path.isfile(p)), RAW_CANDIDATES[0])\n",
    "DATA_DIR = os.path.dirname(RAW_PATH) if os.path.basename(RAW_PATH) else os.path.join('data')\n",
    "CLEAN_PATH = os.path.join(DATA_DIR, 'koi_clean.csv')"
   ],
   "id": "c342242dcae87be4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Column groups (kept consistent with the framework)\n",
    "IDENTIFIER_COLS = [\n",
    "    'kepid',            # Kepler Catalog ID\n",
    "    'kepoi_name',       # KOI Name\n",
    "    'kepler_name',      # Official Kepler Planet Name (if any)\n",
    "]\n",
    "\n",
    "TARGET_COL = 'koi_score'\n",
    "\n",
    "LABEL_COLS = [\n",
    "    'koi_disposition',     # Exoplanet Archive Disposition\n",
    "    'koi_pdisposition',    # Disposition Using Kepler Data\n",
    "    'koi_fpflag_nt',       # Not Transit-Like FP flag\n",
    "    'koi_fpflag_ss',       # Stellar Eclipse FP flag\n",
    "    'koi_fpflag_co',       # Centroid Offset FP flag\n",
    "    'koi_fpflag_ec',       # Ephemeris Match FP flag\n",
    "]\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    # 1) Transit geometry & signal quality\n",
    "    'koi_period', 'koi_impact', 'koi_duration', 'koi_depth', 'koi_model_snr',\n",
    "    # 2) Planet properties & irradiation\n",
    "    'koi_prad', 'koi_teq', 'koi_insol',\n",
    "    # 3) Stellar properties\n",
    "    'koi_steff', 'koi_slogg', 'koi_srad',\n",
    "    # 4) Brightness (observation quality)\n",
    "    'koi_kepmag',\n",
    "]\n",
    "\n",
    "# Always drop from feature matrix (metadata), per framework\n",
    "DROP_ALWAYS = ['koi_tce_plnt_num', 'koi_tce_delivname']\n",
    "\n",
    "ALL_KEEP_COLS = IDENTIFIER_COLS + [TARGET_COL] + LABEL_COLS + FEATURE_COLS"
   ],
   "id": "a939dd1feb017d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_koi(raw_path: str, out_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load raw KOI CSV, select relevant columns, enforce basic validity of koi_score,\n",
    "    and save a clean CSV for downstream use.\n",
    "    \"\"\"\n",
    "    # Load raw CSV; ignore NASA header comments starting with '#'\n",
    "    df = pd.read_csv(raw_path, comment='#', low_memory=False)\n",
    "    orig_rows = len(df)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Keep only the specified columns if present\n",
    "    keep_cols = [c for c in ALL_KEEP_COLS if c in df.columns]\n",
    "    df = df.loc[:, keep_cols].copy()\n",
    "\n",
    "    # Drop exact duplicate rows\n",
    "    before_dupes = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    dropped_dupes = before_dupes - len(df)\n",
    "\n",
    "    # Coerce numeric columns\n",
    "    numeric_cols = set(FEATURE_COLS + [TARGET_COL, 'kepid'])\n",
    "    numeric_cols.update([c for c in LABEL_COLS if c.startswith('koi_fpflag_')])\n",
    "    numeric_cols = [c for c in numeric_cols if c in df.columns]\n",
    "    for c in numeric_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "    # Remove rows with missing target\n",
    "    before_drop_y_na = len(df)\n",
    "    df = df.dropna(subset=[TARGET_COL])\n",
    "    dropped_y_na = before_drop_y_na - len(df)\n",
    "\n",
    "    # Enforce koi_score within [0, 1]\n",
    "    before_range = len(df)\n",
    "    mask_valid = (df[TARGET_COL] >= 0.0) & (df[TARGET_COL] <= 1.0)\n",
    "    df = df.loc[mask_valid].copy()\n",
    "    dropped_out_of_range = before_range - len(df)\n",
    "\n",
    "    # Normalize string columns\n",
    "    for c in ['kepoi_name', 'kepler_name', 'koi_disposition', 'koi_pdisposition']:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype('string').str.strip()\n",
    "\n",
    "    # Save clean CSV\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    df.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f'Loaded rows: {orig_rows}')\n",
    "    print(f'Dropped exact duplicates: {dropped_dupes}')\n",
    "    print(f'Dropped missing koi_score: {dropped_y_na}')\n",
    "    print(f'Dropped out-of-range koi_score: {dropped_out_of_range}')\n",
    "    print(f'Kept columns ({len(keep_cols)}): {keep_cols}')\n",
    "    print(f'Final shape: {df.shape}')\n",
    "    print(f'Saved to: {out_path}')\n",
    "    return df\n",
    "\n",
    "# Run cleaning once to produce data/koi_clean.csv\n",
    "FORCE_REBUILD = False  # set True to overwrite existing clean file if needed\n",
    "if FORCE_REBUILD or (not os.path.exists(CLEAN_PATH)):\n",
    "    _df_clean = clean_koi(RAW_PATH, CLEAN_PATH)\n",
    "else:\n",
    "    print(f'Clean file already exists at {CLEAN_PATH}. Set FORCE_REBUILD=True and re-run this cell to rebuild.')"
   ],
   "id": "590a81aacfa6e331",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the cleaned dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "CLEAN_CANDIDATES = [\n",
    "    os.path.join('data', 'koi_clean.csv'),\n",
    "    os.path.join('..', 'data', 'koi_clean.csv'),\n",
    "]\n",
    "CLEAN_PATH = next((p for p in CLEAN_CANDIDATES if os.path.isfile(p)), CLEAN_CANDIDATES[0])\n",
    "df = pd.read_csv(CLEAN_PATH, low_memory=False)\n",
    "\n",
    "# Re-declare column groups for standalone use of this cell onward\n",
    "IDENTIFIER_COLS = ['kepid', 'kepoi_name', 'kepler_name']\n",
    "TARGET_COL = 'koi_score'\n",
    "LABEL_COLS = ['koi_disposition','koi_pdisposition','koi_fpflag_nt','koi_fpflag_ss','koi_fpflag_co','koi_fpflag_ec']\n",
    "FEATURE_COLS = [\n",
    "    'koi_period','koi_impact','koi_duration','koi_depth','koi_model_snr',\n",
    "    'koi_prad','koi_teq','koi_insol','koi_steff','koi_slogg','koi_srad','koi_kepmag'\n",
    "]\n",
    "\n",
    "print(f'Loaded clean dataset: {df.shape[0]} rows x {df.shape[1]} columns')\n",
    "\n",
    "# Pre-PCA transform specification (fixed per FRAMEWORK)\n",
    "LOG10_COLS = ['koi_period','koi_duration','koi_prad','koi_teq','koi_insol','koi_srad']\n",
    "LOG1P_COLS = ['koi_depth','koi_model_snr']\n",
    "LINEAR_COLS = ['koi_impact','koi_steff','koi_slogg','koi_kepmag']\n",
    "\n",
    "# Build 12-feature matrix and apply transforms\n",
    "feat_cols = LOG10_COLS + LOG1P_COLS + LINEAR_COLS\n",
    "X_pre_pca = df.loc[:, [c for c in feat_cols if c in df.columns]].copy()\n",
    "\n",
    "# Sanity checks\n",
    "missing = [c for c in feat_cols if c not in X_pre_pca.columns]\n",
    "if missing:\n",
    "    print(f'WARNING: missing expected columns: {missing}')\n",
    "\n",
    "for c in LOG10_COLS:\n",
    "    X_pre_pca[c] = np.log10(X_pre_pca[c])\n",
    "for c in LOG1P_COLS:\n",
    "    X_pre_pca[c] = np.log10(X_pre_pca[c] + 1.0)\n",
    "\n",
    "print(f'Prepared pre-PCA feature matrix: {X_pre_pca.shape}')\n",
    "X_pre_pca.head()\n"
   ],
   "id": "862bad669d7678b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": [
     "eda",
     "boxplots-log"
    ]
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use only the 12-feature vector (updated framework)\n",
    "plot_cols = [c for c in FEATURE_COLS if c in df.columns]\n",
    "# Identify discrete small-cardinality numeric columns to skip in box plots\n",
    "discrete_small = [c for c in plot_cols if df[c].nunique(dropna=True) <= 5]\n",
    "numeric_cols = [c for c in plot_cols if c not in discrete_small]\n",
    "\n",
    "# Columns we keep linear due to physical meaning (bounded/magnitude)\n",
    "never_log = {'koi_impact','koi_kepmag'}\n",
    "\n",
    "log10_cols, log1p_cols, linear_cols = [], [], []\n",
    "for col in numeric_cols:\n",
    "    s = pd.to_numeric(df[col], errors='coerce').dropna()\n",
    "    if col in never_log:\n",
    "        linear_cols.append(col)\n",
    "        continue\n",
    "    if len(s) < 3:\n",
    "        linear_cols.append(col)\n",
    "        continue\n",
    "    skew = float(s.skew())\n",
    "    q95, q05 = s.quantile(0.95), s.quantile(0.05)\n",
    "    ratio = (q95 / max(q05, 1e-12)) if q05 > 0 else np.inf\n",
    "    criterion = (skew > 0.75) or (ratio > 20)\n",
    "    minv = float(s.min())\n",
    "    if criterion and minv > 0:\n",
    "        log10_cols.append(col)\n",
    "    elif criterion and minv >= 0:\n",
    "        log1p_cols.append(col)\n",
    "    else:\n",
    "        linear_cols.append(col)\n",
    "\n",
    "# Create transformed copy for plotting\n",
    "plot_df = df.copy()\n",
    "for col in log10_cols:\n",
    "    plot_df[col] = np.log10(plot_df[col])\n",
    "for col in log1p_cols:\n",
    "    plot_df[col] = np.log10(plot_df[col] + 1.0)\n",
    "\n",
    "print('Skipped discrete/binary columns:', discrete_small)\n",
    "print('Log10 columns:', log10_cols)\n",
    "print('Log10(1+x) columns:', log1p_cols)\n",
    "print('Linear columns:', linear_cols)\n",
    "\n",
    "# Plot box plots using transformed data where applicable\n",
    "cols_to_plot = numeric_cols\n",
    "ncols = 4\n",
    "nrows = math.ceil(len(cols_to_plot) / ncols)\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(4.8 * ncols, 3.0 * nrows), squeeze=False)\n",
    "\n",
    "for i, col in enumerate(cols_to_plot):\n",
    "    r, c = divmod(i, ncols)\n",
    "    ax = axes[r][c]\n",
    "    sns.boxplot(x=plot_df[col].dropna(), ax=ax, color='#9467bd', orient='h', whis=1.5, showfliers=False)\n",
    "    tr = ' (log10)' if col in log10_cols else (' (log10(1+x))' if col in log1p_cols else '')\n",
    "    ax.set_title(col + tr)\n",
    "    ax.grid(True, axis='x', linestyle=':', alpha=0.35)\n",
    "\n",
    "# Hide unused axes\n",
    "total_axes = nrows * ncols\n",
    "for j in range(len(cols_to_plot), total_axes):\n",
    "    r, c = divmod(j, ncols)\n",
    "    axes[r][c].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "67789d37dbe0bd19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pre-PCA 12-feature table\n",
    "This is the exact 12-D feature matrix X used for PCA and all models."
   ],
   "id": "c6553ef4c3e1db81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build the 12-feature matrix (order as in FEATURE_COLS) with fixed pre-PCA transforms\n",
    "X_cols = [c for c in FEATURE_COLS if c in df.columns]\n",
    "X_12 = df.loc[:, X_cols].copy()\n",
    "\n",
    "# Ensure transform lists exist (defined earlier)\n",
    "LOG10_COLS = ['koi_period','koi_duration','koi_prad','koi_teq','koi_insol','koi_srad']\n",
    "LOG1P_COLS = ['koi_depth','koi_model_snr']\n",
    "LINEAR_COLS = ['koi_impact','koi_steff','koi_slogg','koi_kepmag']\n",
    "# Apply transforms in-place, keeping original column order\n",
    "for c in LOG10_COLS:\n",
    "    if c in X_12.columns:\n",
    "        X_12[c] = np.log10(X_12[c])\n",
    "for c in LOG1P_COLS:\n",
    "    if c in X_12.columns:\n",
    "        X_12[c] = np.log10(X_12[c] + 1.0)\n",
    "\n",
    "print(f'X_12 shape: {X_12.shape}')\n",
    "X_12.head()\n"
   ],
   "id": "3d4a511f74757e28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Correlation matrix (Pre-PCA X_12)\n",
    "Pearson correlation of the transformed 12 features."
   ],
   "id": "auto_corr_md"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Guard: require X_12 exists\n",
    "if 'X_12' not in globals():\n",
    "    raise RuntimeError('X_12 not found. Run the Pre-PCA 12-feature table cell first.')\n",
    "\n",
    "corr = X_12.corr(numeric_only=True)\n",
    "\n",
    "# Mask upper triangle for cleanliness\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "plt.figure(figsize=(9.5, 7.5))\n",
    "sns.heatmap(corr, mask=mask, cmap='vlag', vmin=-1, vmax=1, center=0,\n",
    "            annot=True, fmt='.2f', linewidths=0.5, cbar_kws=dict(shrink=0.8))\n",
    "plt.title('Correlation matrix of transformed features (X_12)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "auto_corr_code",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PCA: eigen decomposition (manual)\n",
    "Standardize X_12, compute covariance, eigendecomposition, and print eigenvalues and eigenvector matrix A."
   ],
   "id": "pca_eig_md"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if 'X_12' not in globals():\n",
    "    raise RuntimeError('X_12 not found. Run the Pre-PCA 12-feature table cell first.')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X_12.astype(float))\n",
    "\n",
    "# Covariance matrix (features as columns)\n",
    "cov = np.cov(X_std, rowvar=False)\n",
    "\n",
    "# Eigendecomposition of symmetric covariance matrix\n",
    "eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "# Sort by descending eigenvalue\n",
    "order = np.argsort(eigvals)[::-1]\n",
    "eigvals = eigvals[order]\n",
    "eigvecs = eigvecs[:, order]\n",
    "\n",
    "# Eigenvector matrix A: columns are principal axes (PC1..PCm)\n",
    "A = pd.DataFrame(eigvecs, index=X_12.columns, columns=[f'PC{i+1}' for i in range(eigvecs.shape[1])])\n",
    "\n",
    "print('Eigenvalues (descending):')\n",
    "print(pd.Series(eigvals, index=[f'PC{i+1}' for i in range(len(eigvals))]).to_string())\n",
    "print('\\nEigenvector matrix A (loadings):', A)"
   ],
   "id": "pca_eig_code",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PCA: Scree plot and Pareto plot\n",
    "Explained variance per component and cumulative variance."
   ],
   "id": "pca_plots_md"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Require eigvals from previous cell\n",
    "if 'eigvals' not in globals():\n",
    "    raise RuntimeError('Run the PCA eigen decomposition cell first.')\n",
    "\n",
    "expl_var = eigvals / eigvals.sum()\n",
    "cum_var = np.cumsum(expl_var)\n",
    "pcs = np.arange(1, len(eigvals) + 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# Scree\n",
    "axes[0].plot(pcs, eigvals, 'o-', color='#1f77b4')\n",
    "axes[0].set_title('Scree Plot (Eigenvalues)')\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Eigenvalue')\n",
    "axes[0].grid(True, linestyle=':', alpha=0.4)\n",
    "# Pareto\n",
    "axes[1].bar(pcs, expl_var * 100, color='#1f77b4', alpha=0.7, label='Individual %')\n",
    "axes[1].plot(pcs, cum_var * 100, 'o-', color='#d62728', label='Cumulative %')\n",
    "axes[1].set_title('Pareto Plot (Explained Variance)')\n",
    "axes[1].set_xlabel('Principal Component')\n",
    "axes[1].set_ylabel('% Variance Explained')\n",
    "axes[1].set_ylim(0, 110)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, linestyle=':', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "pca_plots_code",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "ax = sns.histplot(df['koi_score'].dropna(), bins=40, kde=True, color='#1f77b4', edgecolor='white')\n",
    "ax.set_title('Distribution of koi_score')\n",
    "ax.set_xlabel('koi_score')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xlim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('koi_score summary:')\n",
    "print(df['koi_score'].describe().to_string())\n"
   ],
   "id": "e04d98be561bcc84",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
